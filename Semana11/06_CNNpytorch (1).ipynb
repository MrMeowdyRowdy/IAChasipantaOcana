{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AW1nn2N_fxEo"
   },
   "source": [
    "# Basic CNN\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u4LVQWLDf5At"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwQ9i17xf7nE"
   },
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r5W2BMZFgFIM"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # CIFAR10 images are color (3 channels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(2304,1024)\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss,\n",
    "        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6wY0NizgLLV"
   },
   "source": [
    "Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_Y_qZaZr_Gn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: -0.007420\n",
      "Train Epoch: 0 [10000/50000 (20%)]\tLoss: -25295712256.000000\n",
      "Train Epoch: 0 [20000/50000 (40%)]\tLoss: -75158401546209722368.000000\n",
      "Train Epoch: 0 [30000/50000 (60%)]\tLoss: -20426487607223544370002132992.000000\n",
      "Train Epoch: 0 [40000/50000 (80%)]\tLoss: -206952875408483742973528375296.000000\n",
      "\n",
      "Test set: Average loss: -385516569369010625330251038720.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: -387502244232606040414048223232.000000\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLoss: -511240637573441794961924161536.000000\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: -639022321096910539741766615040.000000\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: -759375873258552784429112098816.000000\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: -904649466687093735780616503296.000000\n",
      "\n",
      "Test set: Average loss: -1015926727545403874351477948416.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: -1019267345855438098262891954176.000000\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLoss: -1114152231101667772383732170752.000000\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: -1192339357369516462429207265280.000000\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: -1261867023549329289610585440256.000000\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: -1376415993945955618229902639104.000000\n",
      "\n",
      "Test set: Average loss: -1456989637462929947136993787904.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: -1444509042967204510152521678848.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m     25\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, device, train_loader, optimizer, epoch):\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     22\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "train_kwargs = {'batch_size': 500}\n",
    "test_kwargs = {'batch_size': 500}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset1 = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.CIFAR10('../data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(10):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: -0.012847\n",
      "Train Epoch: 0 [10000/50000 (20%)]\tLoss: -4116340224.000000\n",
      "Train Epoch: 0 [20000/50000 (40%)]\tLoss: -11634631822893121536.000000\n",
      "Train Epoch: 0 [30000/50000 (60%)]\tLoss: -4354254473755094217054486528.000000\n",
      "Train Epoch: 0 [40000/50000 (80%)]\tLoss: -342532054910603120015902244864.000000\n",
      "\n",
      "Test set: Average loss: -765223845909978957832576303104.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: -773873009455916513834894884864.000000\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLoss: -1075664944266280163892109770752.000000\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: -1368363036830047669639891124224.000000\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: -1662264237257922909159475380224.000000\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: -1984032084522280062127918022656.000000\n",
      "\n",
      "Test set: Average loss: -2290806495272547058198488547328.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: -2311981622849509169552573857792.000000\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLoss: -2512297759960831639057097293824.000000\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: -2731550964457780015440158785536.000000\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: -2901623253224075855052410978304.000000\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: -3136893515905098453371154595840.000000\n",
      "\n",
      "Test set: Average loss: -3357763772808378558929356980224.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: -3344867744736497243211501142016.000000\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLoss: -3503007028968831981897522348032.000000\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLoss: -3686252377928558628311960387584.000000\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: -3688198144035228373968649977856.000000\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLoss: -3934087006652836067643425292288.000000\n",
      "\n",
      "Test set: Average loss: -4104140074465740848870209880064.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: -4056703516833069446317173899264.000000\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLoss: -4165570611435370743729695490048.000000\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLoss: -4321469151661049767240398798848.000000\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLoss: -4353419852147644801698708324352.000000\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLoss: -4535521255050561105256580644864.000000\n",
      "\n",
      "Test set: Average loss: -4626234013136618822415429926912.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: -4652719057475826812312002494464.000000\n",
      "Train Epoch: 5 [10000/50000 (20%)]\tLoss: -4755801442031272109123555033088.000000\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLoss: -4818863244022739818078340644864.000000\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLoss: -4842223619866608201278481760256.000000\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLoss: -4994977139572559766991186952192.000000\n",
      "\n",
      "Test set: Average loss: -4991440916944019181712013524992.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: -4857365415757281431691676614656.000000\n",
      "Train Epoch: 6 [10000/50000 (20%)]\tLoss: -5009116433727317566161256710144.000000\n",
      "Train Epoch: 6 [20000/50000 (40%)]\tLoss: -5068186967125327576895749881856.000000\n",
      "Train Epoch: 6 [30000/50000 (60%)]\tLoss: -5094057375202170833919874695168.000000\n",
      "Train Epoch: 6 [40000/50000 (80%)]\tLoss: -5141256257051565186158753218560.000000\n",
      "\n",
      "Test set: Average loss: -5246897828763811067760518103040.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: -5268292600028309649095227604992.000000\n",
      "Train Epoch: 7 [10000/50000 (20%)]\tLoss: -5263172799182241694540346949632.000000\n",
      "Train Epoch: 7 [20000/50000 (40%)]\tLoss: -5305130987603606629306874200064.000000\n",
      "Train Epoch: 7 [30000/50000 (60%)]\tLoss: -5309796836804409290606652686336.000000\n",
      "Train Epoch: 7 [40000/50000 (80%)]\tLoss: -5340575483708887942080084574208.000000\n",
      "\n",
      "Test set: Average loss: -5425587989723699814552148377600.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: -5381973335012861496224135512064.000000\n",
      "Train Epoch: 8 [10000/50000 (20%)]\tLoss: -5518696800582177982737530486784.000000\n",
      "Train Epoch: 8 [20000/50000 (40%)]\tLoss: -5540807449360019743028319092736.000000\n",
      "Train Epoch: 8 [30000/50000 (60%)]\tLoss: -5437711464389103781638551109632.000000\n",
      "Train Epoch: 8 [40000/50000 (80%)]\tLoss: -5656840149526631851216617865216.000000\n",
      "\n",
      "Test set: Average loss: -5550580133919260195539682066432.0000, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: -5526705934137124901019958902784.000000\n",
      "Train Epoch: 9 [10000/50000 (20%)]\tLoss: -5564222529097225688198615662592.000000\n",
      "Train Epoch: 9 [20000/50000 (40%)]\tLoss: -5633296319189636948039215087616.000000\n",
      "Train Epoch: 9 [30000/50000 (60%)]\tLoss: -5705176630572283569508894900224.000000\n",
      "Train Epoch: 9 [40000/50000 (80%)]\tLoss: -5720968224090999663103494324224.000000\n",
      "\n",
      "Test set: Average loss: -5638014910438957086277405507584.0000, Accuracy: 1000/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) # CIFAR10 images are color (3 channels)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128) # Adjust input size based on pooling\n",
    "        self.fc2 = nn.Linear(128, 10) # 10 output classes for CIFAR10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Training\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(10):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\ProgramData\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - matplotlib\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    brotli-1.0.9               |       h2bbff1b_8          19 KB\n",
      "    brotli-bin-1.0.9           |       h2bbff1b_8          29 KB\n",
      "    contourpy-1.2.0            |  py310h59b6b97_0         203 KB\n",
      "    fonttools-4.51.0           |  py310h2bbff1b_0         2.0 MB\n",
      "    kiwisolver-1.4.4           |  py310hd77b12b_0          60 KB\n",
      "    libbrotlicommon-1.0.9      |       h2bbff1b_8          79 KB\n",
      "    libbrotlidec-1.0.9         |       h2bbff1b_8          39 KB\n",
      "    libbrotlienc-1.0.9         |       h2bbff1b_8         261 KB\n",
      "    matplotlib-3.8.4           |  py310haa95532_0           9 KB\n",
      "    matplotlib-base-3.8.4      |  py310h4ed8f06_0         8.7 MB\n",
      "    pyparsing-3.0.9            |  py310haa95532_0         154 KB\n",
      "    unicodedata2-15.1.0        |  py310h2bbff1b_0         520 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        12.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  brotli             pkgs/main/win-64::brotli-1.0.9-h2bbff1b_8 \n",
      "  brotli-bin         pkgs/main/win-64::brotli-bin-1.0.9-h2bbff1b_8 \n",
      "  contourpy          pkgs/main/win-64::contourpy-1.2.0-py310h59b6b97_0 \n",
      "  cycler             pkgs/main/noarch::cycler-0.11.0-pyhd3eb1b0_0 \n",
      "  fonttools          pkgs/main/win-64::fonttools-4.51.0-py310h2bbff1b_0 \n",
      "  kiwisolver         pkgs/main/win-64::kiwisolver-1.4.4-py310hd77b12b_0 \n",
      "  libbrotlicommon    pkgs/main/win-64::libbrotlicommon-1.0.9-h2bbff1b_8 \n",
      "  libbrotlidec       pkgs/main/win-64::libbrotlidec-1.0.9-h2bbff1b_8 \n",
      "  libbrotlienc       pkgs/main/win-64::libbrotlienc-1.0.9-h2bbff1b_8 \n",
      "  matplotlib         pkgs/main/win-64::matplotlib-3.8.4-py310haa95532_0 \n",
      "  matplotlib-base    pkgs/main/win-64::matplotlib-base-3.8.4-py310h4ed8f06_0 \n",
      "  pyparsing          pkgs/main/win-64::pyparsing-3.0.9-py310haa95532_0 \n",
      "  unicodedata2       pkgs/main/win-64::unicodedata2-15.1.0-py310h2bbff1b_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "brotli-1.0.9         | 19 KB     |            |   0% \n",
      "\n",
      "fonttools-4.51.0     | 2.0 MB    |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "unicodedata2-15.1.0  | 520 KB    |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "contourpy-1.2.0      | 203 KB    |            |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kiwisolver-1.4.4     | 60 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlidec-1.0.9   | 39 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-3.8.4     | 9 KB      |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlienc-1.0.9   | 261 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlicommon-1.0. | 79 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyparsing-3.0.9      | 154 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "brotli-bin-1.0.9     | 29 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "brotli-1.0.9         | 19 KB     | ########3  |  83% \n",
      "\n",
      "\n",
      "\n",
      "contourpy-1.2.0      | 203 KB    | 7          |   8% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kiwisolver-1.4.4     | 60 KB     | ##6        |  27% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "fonttools-4.51.0     | 2.0 MB    |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "unicodedata2-15.1.0  | 520 KB    | 3          |   3% \u001b[A\u001b[A\n",
      "brotli-1.0.9         | 19 KB     | ########## | 100% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlidec-1.0.9   | 39 KB     | ####       |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kiwisolver-1.4.4     | 60 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-3.8.4     | 9 KB      | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "fonttools-4.51.0     | 2.0 MB    | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "unicodedata2-15.1.0  | 520 KB    | #####8     |  58% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlienc-1.0.9   | 261 KB    | 6          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlicommon-1.0. | 79 KB     | ##         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-3.8.4     | 9 KB      | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlidec-1.0.9   | 39 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "fonttools-4.51.0     | 2.0 MB    | #####5     |  55% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyparsing-3.0.9      | 154 KB    | #          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "brotli-bin-1.0.9     | 29 KB     | #####5     |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlicommon-1.0. | 79 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlicommon-1.0. | 79 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "contourpy-1.2.0      | 203 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "contourpy-1.2.0      | 203 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "unicodedata2-15.1.0  | 520 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    | 2          |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "unicodedata2-15.1.0  | 520 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    | 5          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "brotli-bin-1.0.9     | 29 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "brotli-bin-1.0.9     | 29 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlienc-1.0.9   | 261 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlienc-1.0.9   | 261 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyparsing-3.0.9      | 154 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyparsing-3.0.9      | 154 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    | ###4       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    | #######2   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "fonttools-4.51.0     | 2.0 MB    | ########## | 100% \u001b[A\n",
      "\n",
      "fonttools-4.51.0     | 2.0 MB    | ########## | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.8. | 8.7 MB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.5.2\n",
      "  latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.5.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
